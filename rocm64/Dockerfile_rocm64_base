# Docker Buildfile for ROCm 6.4 to use Ollama with a RX570 / Polaris / gfx803 AMD GPU
# created, build and compiled by Robert Rosenbusch at January-April 2025
# include llm-benchmnark and open-webui 

FROM rocm/dev-ubuntu-24.04:6.4-complete

SHELL ["/bin/bash", "-c"]  
ENV COMMANDLINE_ARGS='' \
    ### how many CPUCores are using while compiling
    MAX_JOBS=14 \ 
    ### Settings for AMD GPU RX570/RX580/RX590 GPU
    HSA_OVERRIDE_GFX_VERSION=8.0.3 \ 
    PYTORCH_ROCM_ARCH=gfx803 \
    ROCM_ARCH=gfx803 \ 
    TORCH_BLAS_PREFER_HIPBLASLT=0\ 
    ROC_ENABLE_PRE_VEGA=1 \
    USE_CUDA=0 \  
    USE_ROCM=1 \ 
    USE_NINJA=1 \
    FORCE_CUDA=1 \ 
    #TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 \
    #PYTORCH_TUNABLEOP_ENABLED=1 \
#######
    DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONENCODING=UTF-8\      
    REQS_FILE='requirements.txt' \
    PIP_ROOT_USER_ACTION='ignore' \
    COMMANDLINE_ARGS='' 

## Write the Environment VARSs to global... to compile later with while you use #docker save# or #docker commit#
RUN echo OLLAMA_HOST=${OLLAMA_HOST} >> /etc/environment && \ 
    echo MAX_JOBS=${MAX_JOBS} >> /etc/environment && \ 
    echo HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION} >> /etc/environment && \ 
    echo PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH} >> /etc/environment && \ 
    echo ROCM_ARCH=${ROCM_ARCH} >> /etc/environment && \ 
    echo TORCH_BLAS_PREFER_HIPBLASLT=${TORCH_BLAS_PREFER_HIPBLASLT} >> /etc/environment && \ 
    echo ROC_ENABLE_PRE_VEGA=${ROC_ENABLE_PRE_VEGA} >> /etc/environment && \
    echo USE_CUDA=${USE_CUDA} >> /etc/environment && \
    echo USE_ROCM=${USE_ROCM} >> /etc/environment && \
    echo USE_NINJA=${USE_NINJA} >> /etc/environment && \
    echo FORCE_CUDA=${FORCE_CUDA} >> /etc/environment && \
    echo PIP_ROOT_USER_ACTION=${PIP_ROOT_USER_ACTION} >> /etc/environment && \
    true

## Export the AMD Stuff
#RUN export OLLAMA_HOST="${OLLAMA_LISTENIP}:${OLLAMA_PORT}" && \
RUN export OLLAMA_HOST=${OLLAMA_HOST} && \
    export MAX_JOBS=${MAX_JOBS} && \ 
    export HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION} && \
    export ROC_ENABLE_PRE_VEGA=${ROC_ENABLE_PRE_VEGA} && \
    export PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH} && \
    export ROCM_ARCH=${ROCM_ARCH} && \
    export TORCH_BLAS_PREFER_HIPBLASLT=${TORCH_BLAS_PREFER_HIPBLASLT} && \    
    export USE_CUDA=${USE_CUDA}  && \   
    export USE_ROCM=${USE_ROCM}  && \
    export USE_NINJA=${USE_NINJA} && \
    export FORCE_CUDA=${FORCE_CUDA} && \
    export PIP_ROOT_USER_ACTION=${PIP_ROOT_USER_ACTION} && \
    true
  
## Update System and install golang for ollama and python virtual Env
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends virtualenv google-perftools ccache tmux mc pigz plocate golang git python3-venv cmake vim wget ffmpeg python3 && \
    apt-get install -y --no-install-recommends libomp-dev libopenblas-dev &&\
    python3 -m venv .venv && \
    . .venv/bin/activate && \
    pip install --upgrade pip wheel && \
    pip install cmake mkl mkl-include && \ 
    true

## Install AMDGPU_TOP to monitor GPU utilization
RUN wget https://github.com/Umio-Yasuno/amdgpu_top/releases/download/v0.10.4/amdgpu-top_0.10.4-1_amd64.deb &&\
    dpkg -i --force-depends amdgpu-top_0.10.4-1_amd64.deb && \
    true

## (Re-)Compile rocBLAS for gfx803    
ENV ROCBLAS_GIT_VERSION="rocm-6.4.0"
RUN echo "Checkout ROCBLAS: ${ROCBLAS_GIT_VERSION}" && \
    git clone https://github.com/ROCm/rocBLAS.git -b ${ROCBLAS_GIT_VERSION} /rocblas && \
    true

WORKDIR /rocblas
RUN echo "BUILDING rocBLAS with ARCH: ${ROCM_ARCH} and JOBS: ${MAX_JOBS}" && \
    ./install.sh -ida ${ROCM_ARCH} -j ${MAX_JOBS} && \
    true

 CMD ["/bin/bash","-c"]