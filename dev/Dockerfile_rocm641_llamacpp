# Docker Buildfile for ROCm 6.4.1 to use LLama.cpp with a RX5(x)0 / Polaris / gfx803 AMD GPU
# created, build and compiled by Robert Rosenbusch at May-April 2025

### Build rocm6_gfx803_base:6.4 first! Read the README.md, it could save lifetime :P
FROM rocm6_gfx803_base:6.4.1
LABEL org.opencontainers.image.authors="Robert Rosenbusch <mail@robert-rosenbusch.de>"
LABEL org.opencontainers.image.url="https://github.com/robertrosenbusch/gfx803_rocm"
LABEL org.opencontainers.image.title="Dockerized ROCm 6.4.0 to use fancy AI Stuff on Ollama/WhisperX/ComfyUI on [GFX803/Polaris/RX5x0]"

ENV COMMANDLINE_ARGS='' 

## Checkout LLama.cpp
RUN echo "Checkout LLama.cpp:" && \
    git clone https://github.com/ggml-org/llama.cpp /llama.cpp  && \
    true 

WORKDIR /llama.cpp

## Compile LLama.cpp for gfx803
RUN HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" cmake -S . -B build -DGGML_HIP=ON -DLLAMA_CURL=OFF -DAMDGPU_TARGETS=${ROCM_ARCH} -DCMAKE_BUILD_TYPE=Release && cmake --build build --config Release -- -j ${MAX_JOBS} && \
    true

CMD ["/bin/bash","-c"]