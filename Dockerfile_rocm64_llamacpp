# Docker Buildfile for ROCm 6.4 to use LLama.cpp with a RX5(x)0 / Polaris / gfx803 AMD GPU
# created, build and compiled by Robert Rosenbusch at May-April 2025

### Build rocm6_gfx803_base:6.4 first! Read the README.md, it could save lifetime :P
FROM rocm6_gfx803_base:6.4

ENV COMMANDLINE_ARGS='' 

## Checkout LLama.cpp
RUN echo "Checkout LLama.cpp:" && \
    git clone https://github.com/ggml-org/llama.cpp /llama.cpp  && \
    true 

WORKDIR /llama.cpp

## Compile LLama.cpp
RUN HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" cmake -S . -B build -DGGML_HIP=ON -DLLAMA_CURL=OFF -DAMDGPU_TARGETS=gfx803 -DCMAKE_BUILD_TYPE=Release && cmake --build build --config Release -- -j ${MAX_JOBS} && \
    true

CMD ["/bin/bash","-c"]