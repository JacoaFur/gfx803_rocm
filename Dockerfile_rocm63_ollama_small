# Docker Buildfile for ROCm 6.3 to use Ollama with a RX570 / Polaris / gfx803 AMD GPU
# created, build and compiled by Robert Rosenbusch at January 2025
# include llm-benchmnark and open-webui 

#FROM rocm/pytorch:rocm6.3_ubuntu24.04_py3.12_pytorch_release_2.4.0
FROM rocm/rocm-terminal:latest


ENV WEBGUI_PORT=8080 \
    OLLAMA_PORT=11434 \
    OLLAMA_HOST=0.0.0.0\
    COMMANDLINE_ARGS='' \
    ### how many CPUCores are using while compiling
    MAX_JOBS=14 \ 
    ### Settings for AMD GPU RX570/RX580/RX590 GPU
    HSA_OVERRIDE_GFX_VERSION=8.0.3 \ 
    PYTORCH_ROCM_ARCH=gfx803 \
    ROCM_ARCH=gfx803 \ 
    TORCH_BLAS_PREFER_HIPBLASLT=0\ 
    ROC_ENABLE_PRE_VEGA=1 \
    USE_CUDA=0 \  
    USE_ROCM=1 \ 
    USE_NINJA=1 \
    FORCE_CUDA=1 \ 
    #TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 \
    #PYTORCH_TUNABLEOP_ENABLED=1 \
#######
    DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONENCODING=UTF-8\      
    REQS_FILE='requirements.txt' \
    PIP_ROOT_USER_ACTION='ignore' \
    COMMANDLINE_ARGS='' 

## Write the Environment VARSs to global... to compile later with while you use #docker save# or #docker commit#
#RUN echo OLLAMA_HOST="${OLLAMA_LISTENIP}:${OLLAMA_PORT}" >> /etc/environment && \ 
RUN sudo -s &&\
#    sudo echo OLLAMA_HOST=${OLLAMA_HOST} >> /etc/environment && \ 
#    echo MAX_JOBS=${MAX_JOBS} >> /etc/environment && \ 
#    echo HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION} >> /etc/environment && \ 
#    echo PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH} >> /etc/environment && \ 
#    echo ROCM_ARCH=${ROCM_ARCH} >> /etc/environment && \ 
#    echo TORCH_BLAS_PREFER_HIPBLASLT=${TORCH_BLAS_PREFER_HIPBLASLT} >> /etc/environment && \ 
#    echo ROC_ENABLE_PRE_VEGA=${ROC_ENABLE_PRE_VEGA} >> /etc/environment && \
#    echo USE_CUDA=${USE_CUDA} >> /etc/environment && \
#    echo USE_ROCM=${USE_ROCM} >> /etc/environment && \
#    echo USE_NINJA=${USE_NINJA} >> /etc/environment && \
#    echo FORCE_CUDA=${FORCE_CUDA} >> /etc/environment && \
    #echo TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=${TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL} >> /etc/environment && \
    #echo PYTORCH_TUNABLEOP_ENABLED=${PYTORCH_TUNABLEOP_ENABLED} >> /etc/environment && \
#    echo PIP_ROOT_USER_ACTION=${PIP_ROOT_USER_ACTION} >> /etc/environment && \
    true

## Export the AMD Stuff
#RUN export OLLAMA_HOST="${OLLAMA_LISTENIP}:${OLLAMA_PORT}" && \
RUN export OLLAMA_HOST=${OLLAMA_HOST} && \
    export MAX_JOBS=${MAX_JOBS} && \ 
    export HSA_OVERRIDE_GFX_VERSION=${HSA_OVERRIDE_GFX_VERSION} && \
    export ROC_ENABLE_PRE_VEGA=${ROC_ENABLE_PRE_VEGA} && \
    export PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH} && \
    export ROCM_ARCH=${ROCM_ARCH} && \
    export TORCH_BLAS_PREFER_HIPBLASLT=${TORCH_BLAS_PREFER_HIPBLASLT} && \    
    export USE_CUDA=${USE_CUDA}  && \
    export USE_ROCM=${USE_ROCM}  && \
    export USE_NINJA=${USE_NINJA} && \
    export FORCE_CUDA=${FORCE_CUDA} && \
    #export TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=${TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL} && \
    #export  PYTORCH_TUNABLEOP_ENABLED=${PYTORCH_TUNABLEOP_ENABLED} && \
    export PIP_ROOT_USER_ACTION=${PIP_ROOT_USER_ACTION} && \
    true

## Update System and install golang for ollama and python virtual Env
RUN sudo apt-get update -y && \
    sudo apt-get install -y --no-install-recommends virtualenv google-perftools ccache tmux mc pigz mlocate golang python3 libssl-dev&& \
    sudo apt-get install -y --no-install-recommends libomp-dev && \
    sudo apt purge -y --no-install-recommends --auto-remove cmake &&\
    sudo apt-get install -y --no-install-recommends  software-properties-common lsb-release &&\
    sudo apt-get install -y --no-install-recommends build-essential libtool autoconf unzip wget &&\
    sudo apt -y clean all &&\
    #pip install --upgrade pip wheel && \
    #pip install cmake mkl mkl-include && \ 
    # symlink for Ollama
    #ln -s /opt/rocm-6.3.4 /opt/rocm && \
    true


 RUN sudo apt-get install -y --no-install-recommends  build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm &&\
    sudo apt-get install -y --no-install-recommends libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl &&\ 
    sudo apt-get install -y --no-install-recommends  gfortran hipblas-common &&\
     sudo apt-get install -y --no-install-recommends apt-utils gcc python3-dev  software-properties-common &&\   
     sudo apt install -y python3-software-properties &&\
     sudo pip3 install psutil &&\
     pip3 install psutil &&\
     sudo apt-get install -y --no-install-recommends software-properties-common &&\
     sudo add-apt-repository -y ppa:deadsnakes/ppa &&\
     sudo apt-get update -y &&\     
     
     sudo apt-get install -y --no-install-recommends python3.13 &&\
     curl -sS https://bootstrap.pypa.io/get-pip.py | python3.13 &&\
    # sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.13 13 &&\
     export PATH=$PATH:/home/rocm-user/.local/bin && \
     true
    
RUN sudo groupadd render &&\
    sudo usermod -a -G render,video root &&\
    sudo usermod -a -G render,video rocm-user &&\
    true

RUN sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test &&\
    sudo apt update &&\
    sudo apt install -y --no-install-recommends gcc-13 g++-13 &&\
    sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-13 13 --slave /usr/bin/g++ g++ /usr/bin/g++-13 &&\
    true

WORKDIR /home/rocm-user
RUN version=3.30.8 && \
    #build=1 &&\
    ## don't modify from here
    mkdir ~/temp &&\
    cd ~/temp &&\
    #https://github.com/Kitware/CMake/releases/download/v3.30.8/cmake-3.30.8.tar.gz
    wget https://github.com/Kitware/CMake/releases/download/v$version/cmake-$version.tar.gz &&\
    tar -xzvf cmake-$version.tar.gz &&\
    cd cmake-$version/ &&\
    ls &&\
    ./bootstrap &&\
    make -j$(nproc) &&\
    sudo make install  &&\
    cmake --version &&\
    true      
#WORKDIR /home/rocm-user
#RUN sudo apt-get install -y --no-install-recommends  build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl &&\
#    wget https://www.python.org/ftp/python/3.12.0/Python-3.12.0.tgz &&\
#    tar -xf Python-3.12.0.tgz &&\
#    cd Python-3.12.0 &&\
#    ./configure --enable-optimizations &&\
 #   make -j 14 && \
 #   sudo make install &&\
#    python3.12 --version &&\
#    echo "**********************" &&\
#    sudo update-alternatives --install /usr/bin/python3 python3 /home/rocm-user/Python-3.12.0/python 12 &&\
#    true

## Compile rocBLAS for gfx803    
ENV ROCBLAS_GIT_VERSION="rocm-6.3.0"
RUN echo "Checkout ROCBLAS: ${ROCBLAS_GIT_VERSION}" && \
    python3 --version &&\
    sudo git clone https://github.com/ROCm/rocBLAS.git -b ${ROCBLAS_GIT_VERSION} /rocblas && \
    true

WORKDIR /rocblas
RUN echo "BUILDING rocBLAS with ARCH: ${ROCM_ARCH} and JOBS: ${MAX_JOBS}" && \
    #sudo pip3 install psutil &&\
    #pip3 install psutil &&\
    sudo apt-get install -y --no-install-recommends gcc &&\
    sudo ./install.sh -ida ${ROCM_ARCH} -j ${MAX_JOBS} --no_hipblaslt && \
    true

RUN git clone https://github.com/ROCm/hipBLAS.git -b rocm-6.3.3 &&\
    true
    

## Checkout interactive LLM Benchmark for Ollama
RUN sudo git clone https://github.com/willybcode/llm-benchmark.git /llm-benchmark && \
    true 

WORKDIR /llm-benchmark
RUN pip install -r requirements.txt && \
    sudo sed -i 's/return \[model\["name"\] for model in models/return \[model\["model"\] for model in models/' benchmark.py&& \ 
    sudo sed -i 's/return OllamaResponse.model_validate(last_element)/return last_element/' benchmark.py &&\
    true

## Install Open WebUI    
WORKDIR / 
#RUN sudo curl -LsSf https://astral.sh/uv/install.sh | sh && \
#    sudo pip install open-webui && \
#    true
 
## Checkout Ollama
ENV OLLAMA_GIT_VERSION="v0.6.2"
RUN echo "Checkout OLLAMA: ${OLLAMA_GIT_VERSION}" && \
    sudo git clone https://github.com/ollama/ollama.git -b ${OLLAMA_GIT_VERSION} /ollama && \
    sudo chown -R rocm-user /ollama &&\
    true

## Replace gfx803 on Ollama    
WORKDIR /ollama
RUN echo "REPLACE Ollama Source for gfx803"  && \
    sudo sed -i 's/var RocmComputeMajorMin = "9"/var RocmComputeMajorMin = "8"/' discover/gpu.go && \
    sudo sed -i 's/"gfx900;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102;gfx1151;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-" /"gfx803;gfx900;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102;gfx1151;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-" /g' CMakePresets.json && \
    sudo sed -i 's/"list(FILTER AMDGPU_TARGETS INCLUDE REGEX "^gfx(900|94[012]|101[02]|1030|110[012])$")"/"list(FILTER AMDGPU_TARGETS INCLUDE REGEX "^gfx(803|900|94[012]|101[02]|1030|110[012])$")"/g' CMakeLists.txt && \
    true

## Compile Ollama    
#RUN echo "BUILDING Ollama for gfx803" && \
#    sudo  ln -s /opt/rocm-6.3.4 /opt/rocm && \
#    sudo apt-get install gfortran hipblas-common &&\
#    cmake -B build -DAMDGPU_TARGETS="${ROCM_ARCH}" && \
#    cmake --build build && \    
#    go generate ./... && \
#    go build . && \
#    true

#RUN touch ol_serve.sh && \
#    echo "export OLLAMA_HOST=0.0.0.0" > ol_serve.sh && \
#    echo "./ollama serve&" >> ol_serve.sh && \
#    chmod +x ol_serve.sh && \
#    true

EXPOSE ${WEBGUI_PORT} ${OLLAMA_PORT}

#ENTRYPOINT ["export OLLAMA_HOST=0.0.0.0 ./ollama serve&"]
#CMD ["/ollama/ol_serve.sh"]

 CMD ["/bin/bash","-c"]